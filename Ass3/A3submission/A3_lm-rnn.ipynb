{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 3. Recurrent Neural Networks for Language Modeling \n",
    "\n",
    "**Total points**: \n",
    "\n",
    "In this assignment, you will train a vanilla RNN-based language model on the Harry Potter text data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from utils import build_training_visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT_RATE = 0.2\n",
    "BATCH_SIZE = 64\n",
    "SEQ_LENGTH = 50\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "MIN_WORD_FREQ = 2\n",
    "\n",
    "DATA_PATH = '/home/stu_12310401/nlp/SUSTech-NLP25/Ass3/Harry_Potter_all_books_preprocessed.txt'\n",
    "MODEL_SAVE_PATH = '/home/stu_12310401/nlp/SUSTech-NLP25/Ass3/rnn_lm_model.pth'\n",
    "VISUALIZATION_PATH = '/home/stu_12310401/nlp/SUSTech-NLP25/Ass3/rnn_lm_training.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    word_counts = Counter(tokens)\n",
    "    vocab = ['<PAD>', '<UNK>', '<START>', '<END>'] + [word for word, count in word_counts.most_common() if count >= MIN_WORD_FREQ]\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    text_indices = []\n",
    "    for token in tokens:\n",
    "        if token in word_to_idx:\n",
    "            text_indices.append(word_to_idx[token])\n",
    "        else:\n",
    "            text_indices.append(word_to_idx['<UNK>'])\n",
    "    \n",
    "    return ' '.join(tokens), text_indices, word_to_idx, idx_to_word, len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_indices, seq_length):\n",
    "        self.text_indices = text_indices\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_indices) - self.seq_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.text_indices[idx:idx+self.seq_length]\n",
    "        target = self.text_indices[idx+1:idx+self.seq_length+1]\n",
    "        \n",
    "        return torch.tensor(sequence, dtype=torch.long), torch.tensor(target, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(x.size(0))\n",
    "            \n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, clip_value=5.0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for inputs, targets in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        hidden = model.init_hidden(inputs.size(0))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        \n",
    "        outputs = outputs.reshape(-1, outputs.size(2))\n",
    "        targets = targets.reshape(-1)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            hidden = model.init_hidden(inputs.size(0))\n",
    "            \n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            \n",
    "            outputs = outputs.reshape(-1, outputs.size(2))\n",
    "            targets = targets.reshape(-1)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_text, word_to_idx, idx_to_word, length=20, temperature=1.0):\n",
    "    model.eval()\n",
    "    \n",
    "    seed_tokens = word_tokenize(seed_text)\n",
    "    \n",
    "    seed_indices = []\n",
    "    for token in seed_tokens:\n",
    "        if token in word_to_idx:\n",
    "            seed_indices.append(word_to_idx[token])\n",
    "        else:\n",
    "            seed_indices.append(word_to_idx['<UNK>'])\n",
    "    \n",
    "    if len(seed_indices) < SEQ_LENGTH:\n",
    "        seed_indices = [word_to_idx['<PAD>']] * (SEQ_LENGTH - len(seed_indices)) + seed_indices\n",
    "    elif len(seed_indices) > SEQ_LENGTH:\n",
    "        seed_indices = seed_indices[-SEQ_LENGTH:]\n",
    "    \n",
    "    current_indices = seed_indices.copy()\n",
    "    generated_tokens = seed_tokens.copy()\n",
    "    \n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            x = torch.tensor([current_indices], dtype=torch.long).to(device)\n",
    "            output, hidden = model(x, hidden)\n",
    "            \n",
    "            output = output[0, -1, :] / temperature\n",
    "            probabilities = torch.softmax(output, dim=0)\n",
    "            \n",
    "            next_index = torch.multinomial(probabilities, 1).item()\n",
    "            \n",
    "            generated_tokens.append(idx_to_word[next_index])\n",
    "            \n",
    "            current_indices = current_indices[1:] + [next_index]\n",
    "    \n",
    "    return ' '.join(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_greedy(model, seed_text, word_to_idx, idx_to_word, length=20):\n",
    "    model.eval()\n",
    "    \n",
    "    seed_tokens = word_tokenize(seed_text)\n",
    "    \n",
    "    seed_indices = []\n",
    "    for token in seed_tokens:\n",
    "        if token in word_to_idx:\n",
    "            seed_indices.append(word_to_idx[token])\n",
    "        else:\n",
    "            seed_indices.append(word_to_idx['<UNK>'])\n",
    "    \n",
    "    if len(seed_indices) < SEQ_LENGTH:\n",
    "        seed_indices = [word_to_idx['<PAD>']] * (SEQ_LENGTH - len(seed_indices)) + seed_indices\n",
    "    elif len(seed_indices) > SEQ_LENGTH:\n",
    "        seed_indices = seed_indices[-SEQ_LENGTH:]\n",
    "    \n",
    "    current_indices = seed_indices.copy()\n",
    "    generated_tokens = seed_tokens.copy()\n",
    "    \n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            x = torch.tensor([current_indices], dtype=torch.long).to(device)\n",
    "            output, hidden = model(x, hidden)\n",
    "            \n",
    "            output = output[0, -1, :]\n",
    "            \n",
    "            next_index = torch.argmax(output).item()\n",
    "            \n",
    "            generated_tokens.append(idx_to_word[next_index])\n",
    "            \n",
    "            current_indices = current_indices[1:] + [next_index]\n",
    "    \n",
    "    return ' '.join(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataloader, desc=\"Calculating Perplexity\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            hidden = model.init_hidden(inputs.size(0))\n",
    "            outputs, _ = model(inputs, hidden)\n",
    "            \n",
    "            outputs = outputs.reshape(-1, outputs.size(2))\n",
    "            targets = targets.reshape(-1)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item() * targets.size(0)\n",
    "            total_words += targets.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / total_words\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Vocabulary size: 18365\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading and preprocessing data...\")\n",
    "text, text_indices, word_to_idx, idx_to_word, vocab_size = load_and_preprocess_data(DATA_PATH)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "dataset = TextDataset(text_indices, SEQ_LENGTH)\n",
    "\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.9 * total_size)\n",
    "val_size = int(0.05 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, \n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT_RATE).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention \n",
    "Because I actually use a .py to run the code, I will load the model directly, but you may need to skip the load cell and train the model .\n",
    "\n",
    "由于我在服务器上训练，为了避免训练中断，我要使用nohup命令。所以实际运行时我将code汇总到一个py脚本来训练模型，并保存checkpoint，所以在notebook上直接加载了权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from /home/stu_12310401/nlp/SUSTech-NLP25/Ass3/rnn_lm_model.pth\n",
      "RNNModel(\n",
      "  (embedding): Embedding(18365, 128)\n",
      "  (rnn): RNN(128, 256, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (fc): Linear(in_features=256, out_features=18365, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "train_loss is 4.019659431076344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2063793/2105598889.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(MODEL_SAVE_PATH)\n"
     ]
    }
   ],
   "source": [
    "# 从检查点加载模型\n",
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    print(f\"Loading checkpoint from {MODEL_SAVE_PATH}\")\n",
    "    checkpoint = torch.load(MODEL_SAVE_PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    train_loss = checkpoint['train_loss']\n",
    "    val_loss = checkpoint['val_loss']\n",
    "    model.eval()  \n",
    "    print(model)\n",
    "    print(f\"train_loss is {train_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skiped\n",
    "print(\"Starting training...\")\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train(model, train_dataloader, criterion, optimizer)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss = evaluate(model, val_dataloader, criterion)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Generate sample text\n",
    "    seed_text = text[:SEQ_LENGTH]\n",
    "    generated_text = generate_text(model, seed_text, word_to_idx, idx_to_word, length=30)\n",
    "    print(f\"Generated Text Sample:\\n{generated_text}\\n\")\n",
    "    \n",
    "    # Save model checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "    }, MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skiped\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.savefig(VISUALIZATION_PATH)\n",
    "\n",
    "train_metrics = {'loss': train_losses}\n",
    "validation_metrics = {'loss': val_losses}\n",
    "build_training_visualization('RNN Language Model', train_metrics, train_losses, validation_metrics, VISUALIZATION_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Generated Text Sample:\n",
      "Harry Potter would he have had to do it in in Harrys mind he felt a thrill of terror .then hatred of having the confusion has been placed upon the office itself in the castle .The last of the castle had been plastered silent and silent but by elves who was now\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"Harry Potter \"\n",
    "generated_text = generate_text(model, seed_text, word_to_idx, idx_to_word, length=50, temperature=0.8)\n",
    "print(f\"Final Generated Text Sample:\\n{generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sentences using greedy search:\n",
      "--------------------------------------------------\n",
      "Prefix: 'Harry look'\n",
      "Generated: 'Harry look as though he had been forced to say that he was not to mention it'\n",
      "--------------------------------------------------\n",
      "Prefix: 'Hermione open'\n",
      "Generated: 'Hermione open the door of the hall and the door opened and Harry saw the sound of'\n",
      "--------------------------------------------------\n",
      "Prefix: 'Ron run'\n",
      "Generated: 'Ron run off the castle and the castle was completely empty and silent as they were all'\n",
      "--------------------------------------------------\n",
      "Prefix: 'Magic is'\n",
      "Generated: 'Magic is a lot of <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>'\n",
      "--------------------------------------------------\n",
      "Prefix: 'Professor Dumbledore'\n",
      "Generated: 'Professor Dumbledore who was unsticking his lemon <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>'\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prefixes=['Harry look','Hermione open','Ron run','Magic is','Professor Dumbledore']\n",
    "print(\"Generating sentences using greedy search:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for prefix in prefixes:\n",
    "    generated_text = generate_text_greedy(model, prefix, word_to_idx, idx_to_word, length=15)\n",
    "    print(f\"Prefix: '{prefix}'\")\n",
    "    print(f\"Generated: '{generated_text}'\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating final perplexity scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity: 100%|██████████| 864/864 [00:05<00:00, 154.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Perplexity Scores:\n",
      "Test Perplexity: 32.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCalculating final perplexity scores...\")\n",
    "# train_perplexity = calculate_perplexity(model, train_dataloader, criterion)\n",
    "# val_perplexity = calculate_perplexity(model, val_dataloader, criterion)\n",
    "test_perplexity = calculate_perplexity(model, test_dataloader, criterion)\n",
    "\n",
    "print(f\"\\nFinal Perplexity Scores:\")\n",
    "# print(f\"Train Perplexity: {train_perplexity:.2f}\")\n",
    "# print(f\"Validation Perplexity: {val_perplexity:.2f}\")\n",
    "print(f\"Test Perplexity: {test_perplexity:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wzh-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
