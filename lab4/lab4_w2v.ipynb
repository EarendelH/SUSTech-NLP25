{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 4 (part 2): Data preparation for implementing word2vec\n",
    "\n",
    "skipgram architecture and negative sampling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pprint import pprint\n",
    "from utils import CorpusReader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary: 1352\n"
     ]
    }
   ],
   "source": [
    "# We set min_count=1 to include all words in the corpus\n",
    "corpus = CorpusReader(inputFileName=\"lunyu_20chapters.txt\", min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "子\n",
      "1352\n"
     ]
    }
   ],
   "source": [
    "print(corpus.word2id[\"子\"])\n",
    "print(corpus.id2word[1])\n",
    "print(len(corpus.id2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient way for negative sampling\n",
    "\n",
    "In `utils.CorpusReader` class, we have implemented a method `initTableNegatives`. It creates a list of words (`self.negatives`) with a size of 1e8. This size is set a large value so that it scales up to very large corpus. \n",
    "\n",
    "The list contains the index of each word in the vocabulary, whose probability is proportional to the power of 0.75 of the word's original frequency count. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simulation of how initTableNegatives works\n",
    "# The impl. in utils.py is a bit different, but the idea is the same\n",
    "word_frequency = {\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4}\n",
    "\n",
    "# the scaled sum of frequencies Z = 1**0.75 + 2**0.75 + 3**0.75 + 4**0.75 = 7.7897270\n",
    "# then the scaled probability of a = 1**0.75 / Z = 0.12837420128374202\n",
    "# the scaled probability of b = 2**0.75 / Z = 0.21589881215898812\n",
    "# the scaled probability of c = 3**0.75 / Z = 0.29262990292629903\n",
    "# the scaled probability of d = 4**0.75 / Z = 0.3630970836309708\n",
    "\n",
    "def initTableNegatives():\n",
    "    pow_frequency = np.array(list(word_frequency.values())) ** 0.75\n",
    "    words_pow = sum(pow_frequency)\n",
    "    ratio = pow_frequency / words_pow\n",
    "    count = np.round(ratio * CorpusReader.NEGATIVE_TABLE_SIZE)\n",
    "    negatives = []\n",
    "    for wid, c in enumerate(count):\n",
    "        negatives += [wid] * int(c)\n",
    "    negatives = np.array(negatives)\n",
    "    np.random.shuffle(negatives)\n",
    "    return negatives\n",
    "\n",
    "negatives = initTableNegatives()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999999\n",
      "{0, 1, 2, 3}\n",
      "0.12837420128374202\n",
      "0.21589881215898812\n",
      "0.29262990292629903\n",
      "0.3630970836309708\n"
     ]
    }
   ],
   "source": [
    "print(len(negatives))\n",
    "print(set(negatives)) # the word indices: a -> 0, b -> 1, c -> 2, d -> 3\n",
    "print(np.sum(negatives == 0) / len(negatives)) # should be the scaled probability of a\n",
    "print(np.sum(negatives == 1) / len(negatives)) # should be the scaled probability of b\n",
    "print(np.sum(negatives == 2) / len(negatives)) # should be the scaled probability of c\n",
    "print(np.sum(negatives == 3) / len(negatives)) # should be the scaled probability of d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the `getNegatives` method returns the negative samples for a target word. The idea is to chop off a segment of given `size` from the `negatives` list. \n",
    "\n",
    "If the segment contains the target word, it is discarded and a new segment is taken. This is done to avoid the target word itself to be sampled as a negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  89, 1345,  605,    3,   27])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test some examples\n",
    "corpus.getNegatives(target=1, size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1. Generate data for training\n",
    "\n",
    "Now we are going to implement the sliding window to generate center, outside, and negative words for each position in a sentence.\n",
    "\n",
    "- It takes a list of words as input and go through each word as a center word.\n",
    "- For each center word, both the left and right `window_size` words are considered as outside words. This number is smaller near the two ends of the sentence.\n",
    "- Call `corpus.getNegatives` to get negative samples for each center word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(words: List[str], window_size: int, k: int, corpus: CorpusReader):\n",
    "    \"\"\" Generate the training data for word2vec skip-gram model\n",
    "    Args:\n",
    "        text: the input text\n",
    "        window_size: the size of the context window\n",
    "        k: the number of negative samples\n",
    "        corpus: the corpus object, providing utilities such as word2id, getNegatives, etc.\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    word_ids = [] # convert the list of words to a list of word ids\n",
    "    # Use for loop and yield\n",
    "    for word in words:\n",
    "        word_ids.append(corpus.word2id[word])\n",
    "\n",
    "    for i in range(len(word_ids)):\n",
    "        center_word = word_ids[i]\n",
    "        context_words = word_ids[max(0, i - window_size):i] + word_ids[i + 1:i + window_size + 1]\n",
    "        for context_word in context_words:\n",
    "            yield center_word, context_word, corpus.getNegatives(center_word, k)\n",
    "    \n",
    "    ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: ['学', '而', '时', '习', '之']\n",
      "word ids: [46, 8, 224, 544, 5]\n",
      "\n",
      "When window size is 3, for center word 学 -> 46\n",
      "the outside words are: \n",
      "而 -> 8\n",
      "时 -> 224\n",
      "习 -> 544\n",
      "\n",
      "output from generate_data:\n",
      "[(46, 8, array([286, 784, 176, 488,   0])),\n",
      " (46, 224, array([   5,  808,    6, 1105,   56])),\n",
      " (46, 544, array([313,  36, 223,   2, 248]))]\n"
     ]
    }
   ],
   "source": [
    "# Test generate_data\n",
    "text = \"学而时习之\"\n",
    "words = list(text)\n",
    "print('words:', words)\n",
    "print('word ids:', [corpus.word2id[word] for word in words])\n",
    "\n",
    "# first center word is 学\n",
    "print()\n",
    "print(f'When window size is 3, for center word 学 -> {corpus.word2id[\"学\"]}')\n",
    "print(f'the outside words are: ')\n",
    "print(f'而 -> {corpus.word2id[\"而\"]}')\n",
    "print(f'时 -> {corpus.word2id[\"时\"]}')\n",
    "print(f'习 -> {corpus.word2id[\"习\"]}')\n",
    "\n",
    "print()\n",
    "print('output from generate_data:')\n",
    "data = list(generate_data(list(text), window_size=3, k=5, corpus=corpus))\n",
    "pprint(data[:3])\n",
    "\n",
    "\n",
    "### You are expected to see the following output:\n",
    "### Note that the negative samples are random, so you may see different numbers\n",
    "# words: ['学', '而', '时', '习', '之']\n",
    "# word ids: [46, 8, 224, 544, 5]\n",
    "\n",
    "# When window size is 3, for center word 学 -> 46\n",
    "# the outside words are: \n",
    "# 而 -> 8\n",
    "# 时 -> 224\n",
    "# 习 -> 544\n",
    "\n",
    "# output from generate_data:\n",
    "# [(46, 8, array([354,   3, 831, 570,  27])),\n",
    "#  (46, 224, array([1077, 1095,   89,  340,   92])),\n",
    "#  (46, 544, array([ 49, 488,   4, 269,  30]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the above data are not in batch. We want all center words are batched into a tensor of dimension `batch_size`; same for the outside words and negative samples.\n",
    "\n",
    "For example, in \"学而时习之\", if `batch_size` is 4, then the returned batch[0] will contain three tensors. \n",
    "- The first tensor contains center words, i.e., 3 \"学\" plus 1 \"而\" => [46, 46, 46, 8]\n",
    "- The second tensor contains the correponding outside words, i.e., \"而\", \"时\", and \"习\" for \"学\"; \"学\" for \"而\" => [8, 224, 544,  46]\n",
    "- The third tensor contains the negative samples, whose dimension is `batch_size` $\\times$ `k`\n",
    "  \n",
    "The data type of the tensors is `torch.long`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data: List, batch_size: int):\n",
    "    \"\"\" Group a stream into batches and yield them as torch tensors.\n",
    "    Args:\n",
    "        data: a list of tuples\n",
    "        batch_size: the batch size \n",
    "    Yields:\n",
    "        a tuple of three torch tensors: center, outside, negative\n",
    "    \"\"\"\n",
    "    assert batch_size < len(data) # data should be long enough\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "        if i > len(data) - batch_size: # if the last batch is smaller than batch_size, pad it with the first few data\n",
    "            batch = batch + data[:i + batch_size - len(data)]\n",
    "        \n",
    "        ### START YOUR CODE ###\n",
    "        centers = []\n",
    "        outsides = []\n",
    "        negatives = []\n",
    "        for center, outside, negative in batch:\n",
    "            centers.append(center)\n",
    "            outsides.append(outside)\n",
    "            negatives.append(negative)\n",
    "        centers = torch.tensor(centers)\n",
    "        outsides = torch.tensor(outsides)\n",
    "        negatives = torch.tensor(negatives)\n",
    "\n",
    "        yield centers, outsides, negatives\n",
    "        ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2. Implement the SkipGram class\n",
    "\n",
    "`SkipGram` is a subclass of `nn.Module`. The two key components are:\n",
    "- `__init__`: initialize the embeddings\n",
    "  - Two `nn.Embedding` objects are created: `self.emb_v` for center words; `self.emb_u` for outside words and negative samples.\n",
    "  - Each `nn.Embedding` is created with `vocab_size` and `emb_dim` as input arguments. \n",
    "  - `self.emb_v` is initialized with uniform distribution; `self.emb_u` is initialized with zeros.\n",
    "- `forward`: given input tensors, return the loss of the model\n",
    "  - Takes three tensors as input: center words, outside words, and negative samples. They are the output from the previously defined `batchify` function.\n",
    "  - Compute the loss using the formula: $-\\log\\sigma(v_c \\cdot u_o) - \\sum_{k=1}^K \\log\\sigma(-v_c \\cdot u_k)$\n",
    "\n",
    "*Hint*:\n",
    "- For the $\\log\\sigma$ function, you can use `F.logsigmoid` in PyTorch. See the imported module: `import torch.nn.functional as F`\n",
    "- If the input to `F.logsigmoid` is too large, it will return 0, which is not good for training. You can use `torch.clamp` to limit the input to a certain range. For example, `torch.clamp(x, min=-10, max=10)` will limit the input to be in the range of $[-10, 10]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.emb_v = nn.Embedding(vocab_size, emb_size, sparse=True)\n",
    "        self.emb_u = nn.Embedding(vocab_size, emb_size, sparse=True)\n",
    "\n",
    "        initrange = 1.0 / self.emb_size # some experience passed down from generation to generation\n",
    "        nn.init.uniform_(self.emb_v.weight.data, -initrange, initrange) # same outcome as self.emb_v.weight.data.uniform_(-initrange, initrange)\n",
    "        nn.init.constant_(self.emb_u.weight.data, 0) # same outcome as self.emb_u.weight.data.zero_()\n",
    "\n",
    "    def forward(self, center, outside, negative):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            center: the center word indices (B, )\n",
    "            outside: the outside word indices (B, )\n",
    "            negative: the negative word indices (B, k)\n",
    "        \"\"\"\n",
    "        v_c = self.emb_v(center)\n",
    "        u_o = self.emb_u(outside)\n",
    "        u_n = self.emb_u(negative)\n",
    "        \n",
    "        ### START YOUR CODE ###\n",
    "        pos_loss = -F.logsigmoid(torch.sum(v_c * u_o, dim=1))\n",
    "        neg_score = torch.sum(v_c.unsqueeze(1) * u_n, dim=2)\n",
    "        neg_score_clamped = torch.clamp(neg_score, min=-100, max=100)\n",
    "        neg_loss = -F.logsigmoid(-neg_score_clamped)\n",
    "        neg_loss = torch.sum(neg_loss, dim=1)\n",
    "        loss = pos_loss + neg_loss\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def save_embedding(self, id2word, file_name):\n",
    "        embedding = self.emb_v.weight.cpu().data.numpy()\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write('%d %d\\n' % (len(id2word), self.emb_size))\n",
    "            for wid, w in id2word.items():\n",
    "                e = ' '.join(map(lambda x: str(x), embedding[wid]))\n",
    "                f.write('%s %s\\n' % (w, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.1814, 4.2008, 4.2172, 4.2307, 4.2414])\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "vacob_size =len(corpus.id2word)\n",
    "emb_size = 32\n",
    "model = SkipGram(vacob_size, emb_size)\n",
    "\n",
    "weight = torch.empty(vacob_size, emb_size)\n",
    "start_value = 0.01\n",
    "for i in range(vacob_size):\n",
    "    weight[i] = start_value + i * 0.01\n",
    "\n",
    "model.emb_v.weight.data.copy_(weight)\n",
    "model.emb_u.weight.data.copy_(weight)\n",
    "\n",
    "# Test the model\n",
    "center = torch.tensor([0, 1, 2, 3, 4])\n",
    "outside = torch.tensor([0, 1, 2, 3, 4])\n",
    "negative = torch.tensor([[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]])\n",
    "with torch.no_grad():\n",
    "    loss = model(center, outside, negative)\n",
    "print(loss)\n",
    "\n",
    "\n",
    "### You are expected to see the following output:\n",
    "# tensor([4.1814, 4.2008, 4.2172, 4.2307, 4.2414])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_embedding(corpus.id2word,file_name=\"id2word.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from matplotlib import pyplot as plt\n",
    "# embeddings = model.emb_v.weight.cpu().data.numpy()\n",
    "\n",
    "\n",
    "words = ['学', '习', '曰', '子', '人', '仁']\n",
    "words_pinyin = ['xue', 'xi', 'yue', 'zi', 'ren1', 'ren2']\n",
    "# svd = TruncatedSVD(n_components=2)\n",
    "# # M=svd.fit_transform(embeddings)\n",
    "# M = svd.fit_transform(embeddings)\n",
    "# ### YOUR CODE HERE ###\n",
    "# plt.figure(figsize=(6, 6))\n",
    "# for i, word in enumerate(words):\n",
    "#     plt.scatter(M[corpus.word2id[word], 0], M[corpus.word2id[word], 1])\n",
    "#     plt.text(M[corpus.word2id[word], 0], M[corpus.word2id[word], 1], words_pinyin[i])\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'，': 0, '子': 1, '。': 2, '：': 3, '曰': 4, '之': 5, '不': 6, '也': 7, '而': 8, '？': 9, '其': 10, '人': 11, '者': 12, '以': 13, '有': 14, '于': 15, '矣': 16, '为': 17, '君': 18, '乎': 19, '可': 20, '如': 21, '与': 22, '；': 23, '言': 24, '无': 25, '则': 26, '！': 27, '问': 28, '知': 29, '何': 30, '吾': 31, '仁': 32, '夫': 33, '、': 34, '道': 35, '焉': 36, '行': 37, '谓': 38, '必': 39, '礼': 40, '孔': 41, '斯': 42, '三': 43, '能': 44, '见': 45, '学': 46, '事': 47, '是': 48, '哉': 49, '闻': 50, '未': 51, '公': 52, '好': 53, '我': 54, '路': 55, '在': 56, '已': 57, '得': 58, '所': 59, '民': 60, '小': 61, '天': 62, '乐': 63, '邦': 64, '亦': 65, '使': 66, '大': 67, '下': 68, '贡': 69, '欲': 70, '诸': 71, '从': 72, '文': 73, '政': 74, '食': 75, '善': 76, '后': 77, '德': 78, '求': 79, '对': 80, '恶': 81, '信': 82, '死': 83, '然': 84, '《': 85, '》': 86, '由': 87, '仲': 88, '非': 89, '出': 90, '过': 91, '一': 92, '虽': 93, '足': 94, '居': 95, '尔': 96, '夏': 97, '父': 98, '及': 99, '己': 100, '予': 101, '色': 102, '友': 103, '成': 104, '远': 105, '上': 106, '立': 107, '今': 108, '张': 109, '中': 110, '季': 111, '臣': 112, '贤': 113, '思': 114, '齐': 115, '义': 116, '正': 117, '命': 118, '曾': 119, '伯': 120, '门': 121, '敢': 122, '犹': 123, '生': 124, '日': 125, '敬': 126, '难': 127, '直': 128, '丧': 129, '说': 130, '自': 131, '用': 132, '周': 133, '颜': 134, '年': 135, '告': 136, '十': 137, '怨': 138, '达': 139, '孝': 140, '弟': 141, '至': 142, '先': 143, '师': 144, '女': 145, '多': 146, '忠': 147, '四': 148, '疾': 149, '服': 150, '氏': 151, '冉': 152, '将': 153, '士': 154, '丘': 155, '身': 156, '入': 157, '志': 158, '耻': 159, '失': 160, '安': 161, '富': 162, '赐': 163, '患': 164, '百': 165, '回': 166, '宰': 167, '朝': 168, '莫': 169, '若': 170, '云': 171, '孙': 172, '或': 173, '勇': 174, '孰': 175, '叔': 176, '渊': 177, '方': 178, '乱': 179, '五': 180, '唯': 181, '忧': 182, '马': 183, '皆': 184, '故': 185, '世': 186, '语': 187, '又': 188, '衣': 189, '改': 190, '美': 191, '诗': 192, '违': 193, '祭': 194, '武': 195, '游': 196, '举': 197, '既': 198, '二': 199, '执': 200, '进': 201, '称': 202, '众': 203, '力': 204, '勿': 205, '恭': 206, '退': 207, '去': 208, '管': 209, '请': 210, '易': 211, '且': 212, '哀': 213, '益': 214, '取': 215, '古': 216, '鲁': 217, '党': 218, '适': 219, '惠': 220, '南': 221, '陈': 222, '谁': 223, '时': 224, '作': 225, '终': 226, '归': 227, '异': 228, '观': 229, '近': 230, '往': 231, '孟': 232, '樊': 233, '寡': 234, '奚': 235, '兄': 236, '家': 237, '夷': 238, '亡': 239, '长': 240, '利': 241, '躬': 242, '明': 243, '乡': 244, '来': 245, '令': 246, '国': 247, '母': 248, '固': 249, '惑': 250, '六': 251, '迟': 252, '劳': 253, '相': 254, '辟': 255, '尝': 256, '久': 257, '苟': 258, '次': 259, '佞': 260, '室': 261, '月': 262, '修': 263, '卫': 264, '畏': 265, '朋': 266, '谋': 267, '乘': 268, '爱': 269, '亲': 270, '致': 271, '复': 272, '宗': 273, '敏': 274, '贫': 275, '蔽': 276, '愚': 277, '泰': 278, '山': 279, '同': 280, '反': 281, '怀': 282, '老': 283, '厌': 284, '隐': 285, '兴': 286, '召': 287, '杀': 288, '和': 289, '贵': 290, '骄': 291, '视': 292, '康': 293, '车': 294, '庙': 295, '名': 296, '加': 297, '位': 298, '内': 299, '容': 300, '废': 301, '仕': 302, '听': 303, '狂': 304, '简': 305, '少': 306, '毋': 307, '质': 308, '圣': 309, '舜': 310, '病': 311, '桓': 312, '岂': 313, '止': 314, '受': 315, '慎': 316, '让': 317, '王': 318, '始': 319, '譬': 320, '比': 321, '枉': 322, '教': 323, '殷': 324, '升': 325, '神': 326, '间': 327, '惧': 328, '赤': 329, '愿': 330, '裘': 331, '伐': 332, '弓': 333, '牛': 334, '费': 335, '变': 336, '博': 337, '司': 338, '荡': 339, '笃': 340, '才': 341, '危': 342, '冕': 343, '鲜': 344, '节': 345, '俭': 346, '没': 347, '辱': 348, '切': 349, '免': 350, '七': 351, '心': 352, '器': 353, '临': 354, '庄': 355, '施': 356, '损': 357, '宁': 358, '弗': 359, '贾': 360, '罪': 361, '太': 362, '尽': 363, '战': 364, '处': 365, '约': 366, '各': 367, '治': 368, '墙': 369, '臧': 370, '旧': 371, '辞': 372, '舍': 373, '闵': 374, '手': 375, '动': 376, '识': 377, '肉': 378, '饭': 379, '期': 380, '姓': 381, '气': 382, '谷': 383, '巍': 384, '拜': 385, '趋': 386, '末': 387, '怡': 388, '徒': 389, '尼': 390, '本': 391, '巧': 392, '千': 393, '余': 394, '主': 395, '厚': 396, '抑': 397, '温': 398, '因': 399, '刑': 400, '酒': 401, '诲': 402, '禄': 403, '阙': 404, '书': 405, '鬼': 406, '雍': 407, '堂': 408, '放': 409, '射': 410, '饮': 411, '笑': 412, '兮': 413, '征': 414, '谏': 415, '木': 416, '里': 417, '盖': 418, '几': 419, '喜': 420, '数': 421, '妻': 422, '宾': 423, '寝': 424, '刚': 425, '尹': 426, '弃': 427, '他': 428, '希': 429, '微': 430, '侍': 431, '华': 432, '九': 433, '骞': 434, '胜': 435, '尧': 436, '倦': 437, '甚': 438, '衰': 439, '歌': 440, '忘': 441, '恒': 442, '宿': 443, '厉': 444, '鄙': 445, '夺': 446, '任': 447, '守': 448, '禹': 449, '玉': 450, '阶': 451, '坐': 452, '鲤': 453, '贼': 454, '侯': 455, '景': 456, '草': 457, '矜': 458, '颛': 459, '臾': 460, '戒': 461, '犯': 462, '省': 463, '交': 464, '重': 465, '威': 466, '共': 467, '逾': 468, '御': 469, '葬': 470, '养': 471, '发': 472, '察': 473, '殆': 474, '攻': 475, '干': 476, '错': 477, '舞': 478, '庭': 479, '旅': 480, '揖': 481, '商': 482, '祷': 483, '羊': 484, '伤': 485, '官': 486, '宽': 487, '择': 488, '贱': 489, '疏': 490, '海': 491, '章': 492, '恐': 493, '弑': 494, '高': 495, '左': 496, '幸': 497, '粟': 498, '果': 499, '艺': 500, '偃': 501, '水': 502, '觚': 503, '欺': 504, '畔': 505, '军': 506, '雅': 507, '童': 508, '西': 509, '鸟': 510, '昔': 511, '弘': 512, '宫': 513, '绝': 514, '匡': 515, '前': 516, '沽': 517, '待': 518, '困': 519, '便': 520, '侃': 521, '訚': 522, '鱼': 523, '席': 524, '杖': 525, '东': 526, '风': 527, '恸': 528, '瑟': 529, '鼓': 530, '庶': 531, '崇': 532, '硁': 533, '偲': 534, '谅': 535, '报': 536, '穷': 537, '声': 538, '柳': 539, '群': 540, '毁': 541, '阳': 542, '逸': 543, '习': 544, '愠': 545, '谨': 546, '竭': 547, '饱': 548, '就': 549, '谄': 550, '耳': 551, '顺': 552, '新': 553, '罔': 554, '端': 555, '疑': 556, '尤': 557, '悔': 558, '惟': 559, '继': 560, '忍': 561, '彻': 562, '戚': 563, '争': 564, '素': 565, '每': 566, '定': 567, '社': 568, '摄': 569, '绎': 570, '韶': 571, '尚': 572, '参': 573, '贯': 574, '邻': 575, '屡': 576, '邑': 577, '望': 578, '申': 579, '产': 580, '平': 581, '蔡': 582, '崔': 583, '讼': 584, '面': 585, '乃': 586, '短': 587, '陋': 588, '野': 589, '史': 590, '逝': 591, '矢': 592, '述': 593, '窃': 594, '启': 595, '暴': 596, '虎': 597, '河': 598, '诺': 599, '叶': 600, '洁': 601, '巫': 602, '猛': 603, '履': 604, '貌': 605, '慢': 606, '功': 607, '虞': 608, '分': 609, '空': 610, '凤': 611, '瞽': 612, '忽': 613, '循': 614, '诈': 615, '夜': 616, '地': 617, '匹': 618, '狐': 619, '权': 620, '似': 621, '勃': 622, '鞠': 623, '降': 624, '授': 625, '亵': 626, '当': 627, '羔': 628, '冠': 629, '量': 630, '俟': 631, '椁': 632, '噫': 633, '稷': 634, '皙': 635, '哂': 636, '会': 637, '克': 638, '讱': 639, '独': 640, '润': 641, '肤': 642, '愬': 643, '兵': 644, '盗': 645, '虑': 646, '忿': 647, '稼': 648, '速': 649, '彼': 650, '优': 651, '寮': 652, '肆': 653, '击': 654, '磬': 655, '舆': 656, '郑': 657, '火': 658, '血': 659, '桀': 660, '溺': 661, '拒': 662, '尊': 663, '帝': 664, '务': 665, '传': 666, '惮': 667, '追': 668, '禽': 669, '磨': 670, '犬': 671, '别': 672, '馔': 673, '私': 674, '廋': 675, '害': 676, '劝': 677, '八': 678, '穆': 679, '林': 680, '奢': 681, '狄': 682, '目': 683, '宋': 684, '禘': 685, '指': 686, '媚': 687, '获': 688, '代': 689, '郁': 690, '关': 691, '雎': 692, '淫': 693, '松': 694, '柏': 695, '栗': 696, '遂': 697, '树': 698, '塞': 699, '两': 700, '坫': 701, '纯': 702, '颠': 703, '议': 704, '土': 705, '恕': 706, '喻': 707, '逮': 708, '讷': 709, '孤': 710, '口': 711, '雕': 712, '浮': 713, '束': 714, '客': 715, '愈': 716, '昼': 717, '枨': 718, '性': 719, '晏': 720, '清': 721, '再': 722, '乞': 723, '盍': 724, '轻': 725, '敝': 726, '迁': 727, '原': 728, '川': 729, '巷': 730, '儒': 731, '城': 732, '灭': 733, '祝': 734, '鮀': 735, '户': 736, '彬': 737, '徙': 738, '夭': 739, '愤': 740, '隅': 741, '侧': 742, '哭': 743, '藏': 744, '图': 745, '虚': 746, '盈': 747, '败': 748, '昭': 749, '吴': 750, '绞': 751, '兢': 752, '深': 753, '薄': 754, '鸣': 755, '豆': 756, '实': 757, '毅': 758, '吝': 759, '挚': 760, '洋': 761, '悾': 762, '唐': 763, '盛': 764, '妇': 765, '沟': 766, '纵': 767, '试': 768, '叩': 769, '裳': 770, '喟': 771, '叹': 772, '仰': 773, '弥': 774, '钻': 775, '坚': 776, '瞻': 777, '椟': 778, '篑': 779, '覆': 780, '惜': 781, '秀': 782, '法': 783, '帅': 784, '貉': 785, '诵': 786, '岁': 787, '恂': 788, '踧': 789, '踖': 790, '躩': 791, '右': 792, '翼': 793, '顾': 794, '屏': 795, '圭': 796, '蹜': 797, '愉': 798, '饰': 799, '紫': 800, '缁': 801, '玄': 802, '馁': 803, '割': 804, '市': 805, '瓜': 806, '馈': 807, '首': 808, '绅': 809, '狎': 810, '式': 811, '负': 812, '绥': 813, '白': 814, '货': 815, '亿': 816, '论': 817, '饥': 818, '点': 819, '春': 820, '浴': 821, '雩': 822, '承': 823, '浸': 824, '谮': 825, '驷': 826, '鞟': 827, '辨': 828, '诚': 829, '慝': 830, '选': 831, '汤': 832, '赦': 833, '罚': 834, '圃': 835, '情': 836, '合': 837, '斗': 838, '狷': 839, '备': 840, '即': 841, '讨': 842, '绰': 843, '要': 844, '谲': 845, '纠': 846, '到': 847, ',': 848, '灵': 849, '蘧': 850, '栖': 851, '荷': 852, '幼': 853, '并': 854, '工': 855, '誉': 856, '耕': 857, '莅': 858, '蹈': 859, '某': 860, '均': 861, '崩': 862, '侮': 863, '亢': 864, '遇': 865, '涂': 866, '鸡': 867, '佛': 868, '肸': 869, '兽': 870, '朱': 871, '流': 872, '黜': 873, '楚': 874, '沮': 875, '津': 876, '滔': 877, '丈': 878, '伦': 879, '连': 880, '俨': 881, '皇': 882, '朕': 883, '万': 884, '贪': 885, '泛': 886, '良': 887, '磋': 888, '琢': 889, '北': 890, '辰': 891, '星': 892, '邪': 893, '格': 894, '矩': 895, '懿': 896, '慈': 897, '輗': 898, '軏': 899, '佾': 900, '维': 901, '救': 902, '呜': 903, '呼': 904, '倩': 905, '盼': 906, '绚': 907, '绘': 908, '起': 909, '杞': 910, '献': 911, '灌': 912, '示': 913, '掌': 914, '奥': 915, '灶': 916, '监': 917, '鄹': 918, '皮': 919, '科': 920, '朔': 921, '饩': 922, '咎': 923, '翕': 924, '皦': 925, '仪': 926, '封': 927, '铎': 928, '造': 929, '沛': 930, '夕': 931, '冶': 932, '缧': 933, '绁': 934, '戮': 935, '瑚': 936, '琏': 937, '给': 938, '憎': 939, '漆': 940, '开': 941, '桴': 942, '材': 943, '赋': 944, '带': 945, '朽': 946, '粪': 947, '圬': 948, '诛': 949, '藻': 950, '棁': 951, '斐': 952, '裁': 953, '念': 954, '醯': 955, '匿': 956, '憾': 957, '桑': 958, '怒': 959, '贰': 960, '釜': 961, '庾': 962, '秉': 963, '肥': 964, '急': 965, '犁': 966, '骍': 967, '角': 968, '汶': 969, '牖': 970, '箪': 971, '瓢': 972, '堪': 973, '画': 974, '澹': 975, '台': 976, '径': 977, '奔': 978, '殿': 979, '策': 980, '静': 981, '寿': 982, '井': 983, '陷': 984, '否': 985, '庸': 986, '济': 987, '彭': 988, '默': 989, '讲': 990, '燕': 991, '梦': 992, '据': 993, '依': 994, '脩': 995, '悱': 996, '冯': 997, '鞭': 998, '斋': 999, '味': 1000, '於': 1001, '曲': 1002, '肱': 1003, '枕': 1004, '怪': 1005, '魋': 1006, '钓': 1007, '纲': 1008, '弋': 1009, '互': 1010, '保': 1011, '诔': 1012, '祇': 1013, '坦': 1014, '葸': 1015, '遗': 1016, '偷': 1017, '冰': 1018, '倍': 1019, '笾': 1020, '存': 1021, '校': 1022, '托': 1023, '尺': 1024, '寄': 1025, '侗': 1026, '焕': 1027, '际': 1028, '菲': 1029, '黼': 1030, '卑': 1031, '洫': 1032, '罕': 1033, '麻': 1034, '意': 1035, '兹': 1036, '牢': 1037, '诱': 1038, '罢': 1039, '卓': 1040, '韫': 1041, '颂': 1042, '卿': 1043, '勉': 1044, '惰': 1045, '苗': 1046, '巽': 1047, '缊': 1048, '袍': 1049, '忮': 1050, '寒': 1051, '凋': 1052, '棣': 1053, '偏': 1054, '摈': 1055, '襜': 1056, '阈': 1057, '息': 1058, '等': 1059, '逞': 1060, '享': 1061, '觌': 1062, '绀': 1063, '緅': 1064, '红': 1065, '暑': 1066, '袗': 1067, '絺': 1068, '绤': 1069, '表': 1070, '麑': 1071, '黄': 1072, '袂': 1073, '半': 1074, '佩': 1075, '帷': 1076, '吊': 1077, '吉': 1078, '布': 1079, '精': 1080, '脍': 1081, '细': 1082, '饐': 1083, '餲': 1084, '臭': 1085, '饪': 1086, '酱': 1087, '脯': 1088, '撤': 1089, '姜': 1090, '菜': 1091, '羹': 1092, '傩': 1093, '阼': 1094, '送': 1095, '药': 1096, '厩': 1097, '焚': 1098, '腥': 1099, '熟': 1100, '荐': 1101, '畜': 1102, '拖': 1103, '驾': 1104, '殡': 1105, '尸': 1106, '凶': 1107, '版': 1108, '迅': 1109, '雷': 1110, '烈': 1111, '翔': 1112, '集': 1113, '梁': 1114, '雌': 1115, '雉': 1116, '嗅': 1117, '助': 1118, '昆': 1119, '棺': 1120, '府': 1121, '仍': 1122, '聚': 1123, '敛': 1124, '附': 1125, '柴': 1126, '喭': 1127, '殖': 1128, '践': 1129, '迹': 1130, '兼': 1131, '具': 1132, '读': 1133, '率': 1134, '馑': 1135, '甫': 1136, '铿': 1137, '撰': 1138, '暮': 1139, '沂': 1140, '咏': 1141, '疚': 1142, '棘': 1143, '舌': 1144, '豹': 1145, '只': 1146, '片': 1147, '折': 1148, '狱': 1149, '赏': 1150, '皋': 1151, '陶': 1152, '伊': 1153, '辅': 1154, '迂': 1155, '措': 1156, '农': 1157, '须': 1158, '襁': 1159, '专': 1160, '荆': 1161, '完': 1162, '仆': 1163, '残': 1164, '莒': 1165, '攘': 1166, '证': 1167, '族': 1168, '筲': 1169, '算': 1170, '医': 1171, '羞': 1172, '占': 1173, '戎': 1174, '宪': 1175, '羿': 1176, '奡': 1177, '舟': 1178, '俱': 1179, '答': 1180, '裨': 1181, '谌': 1182, '创': 1183, '羽': 1184, '骈': 1185, '齿': 1186, '赵': 1187, '魏': 1188, '滕': 1189, '薛': 1190, '卞': 1191, '防': 1192, '晋': 1193, '霸': 1194, '被': 1195, '衽': 1196, '经': 1197, '渎': 1198, '僎': 1199, '圉': 1200, '怍': 1201, '沐': 1202, '暇': 1203, '逆': 1204, '觉': 1205, '亩': 1206, '骥': 1207, '石': 1208, '晨': 1209, '蒉': 1210, '浅': 1211, '揭': 1212, '阴': 1213, '薨': 1214, '总': 1215, '冢': 1216, '壤': 1217, '胫': 1218, '俎': 1219, '粮': 1220, '滥': 1221, '蛮': 1222, '貊': 1223, '州': 1224, '倚': 1225, '衡': 1226, '卷': 1227, '辂': 1228, '责': 1229, '慧': 1230, '借': 1231, '贞': 1232, '类': 1233, '蒙': 1234, '域': 1235, '列': 1236, '持': 1237, '扶': 1238, '兕': 1239, '柙': 1240, '龟': 1241, '倾': 1242, '离': 1243, '析': 1244, '戈': 1245, '萧': 1246, '陪': 1247, '柔': 1248, '佚': 1249, '宴': 1250, '愆': 1251, '躁': 1252, '壮': 1253, '聪': 1254, '探': 1255, '饿': 1256, '豚': 1257, '宝': 1258, '迷': 1259, '亟': 1260, '移': 1261, '弦': 1262, '莞': 1263, '刀': 1264, '戏': 1265, '扰': 1266, '牟': 1267, '磷': 1268, '涅': 1269, '匏': 1270, '系': 1271, '迩': 1272, '帛': 1273, '钟': 1274, '荏': 1275, '穿': 1276, '窬': 1277, '廉': 1278, '戾': 1279, '物': 1280, '孺': 1281, '悲': 1282, '坏': 1283, '燧': 1284, '稻': 1285, '锦': 1286, '旨': 1287, '甘': 1288, '通': 1289, '弈': 1290, '讪': 1291, '窒': 1292, '徼': 1293, '讦': 1294, '逊': 1295, '箕': 1296, '奴': 1297, '接': 1298, '耦': 1299, '耰': 1300, '辍': 1301, '怃': 1302, '蓧': 1303, '体': 1304, '勤': 1305, '植': 1306, '芸': 1307, '拱': 1308, '黍': 1309, '亚': 1310, '缭': 1311, '缺': 1312, '秦': 1313, '播': 1314, '鼗': 1315, '汉': 1316, '襄': 1317, '突': 1318, '随': 1319, '騧': 1320, '嘉': 1321, '泥': 1322, '谤': 1323, '闲': 1324, '洒': 1325, '扫': 1326, '应': 1327, '区': 1328, '诬': 1329, '卒': 1330, '散': 1331, '纣': 1332, '更': 1333, '坠': 1334, '常': 1335, '肩': 1336, '窥': 1337, '仞': 1338, '宜': 1339, '陵': 1340, '荣': 1341, '咨': 1342, '历': 1343, '允': 1344, '永': 1345, '牡': 1346, '赉': 1347, '审': 1348, '度': 1349, '虐': 1350, '纳': 1351}\n",
      "[[ 2.6587219e+00 -9.7206652e-08]\n",
      " [ 3.0829847e+01 -6.7425492e-07]\n",
      " [ 2.8284267e-01 -5.6025757e-09]\n",
      " [ 1.1313707e-01 -2.2263902e-09]\n",
      " [ 6.7882282e-01 -2.7561009e-08]\n",
      " [ 1.8667613e+00  5.7027973e-09]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gensim\n",
    "embedding_file = \"id2word.txt\"\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(embedding_file, binary=False)\n",
    "print(model.key_to_index)\n",
    "selected_embeddings = np.array([model[word] for word in words])\n",
    "# print(selected_embeddings)\n",
    "svd1 = TruncatedSVD(n_components=2, random_state=42)\n",
    "M1 = svd1.fit_transform(selected_embeddings)\n",
    "print(M1)\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, word in enumerate(words):\n",
    "    plt.scatter(M1[i, 0], M1[i, 1])\n",
    "    plt.annotate(words_pinyin[i], (M1[i, 0], M1[i, 1]))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
